{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import ntpath\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import collections\n",
    "from spotlight import annotate\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "\n",
    "#A function for Slicing a dictionary\n",
    "'''n is the slice rate \n",
    "iterable -> the dictionary to be sliced. \n",
    "convert the dictionary to a list and use islice function to slice it\n",
    "'''\n",
    "def dicslice(n, iterable):\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "#nltk.download('punkt') # if necessary...\n",
    "''' a stemmer which would be used to reduce \n",
    "each word to its root equivalence is built.\n",
    "this will help reduce the noise in the text document.\n",
    "this is also built alongside punctuation removal.\n",
    "'''\n",
    "\n",
    "'''Function that creates tokens to use.'''\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words=STOPWORDS)    \n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "def ProjectTrust(targetP):\n",
    "    \n",
    "    targetp = ntpath.basename(targetP)\n",
    "    testfile = targetp\n",
    "\n",
    "    newProjectReadme = []\n",
    "    with open(r\"githubtestreadme/\"+testfile,'r') as newPorject:\n",
    "        dataNew = newPorject.read()\n",
    "        newProjectReadme.append(dataNew.replace(\"\\n\",\" \"))\n",
    "\n",
    "    '''for both the new and old readme files, append the name of the files to a \n",
    "    list containing the readme texts for each document'''\n",
    "    path = \"githubreadmefiles/*\"\n",
    "    oldProjectReadme = []\n",
    "    readmewithname = []\n",
    "    for fname in glob.glob(path):\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = infile.read()\n",
    "            oldProjectReadme.append(data.replace(\"\\n\",\"\"))\n",
    "            readmewithname.append((fname,data.replace(\"\\n\",\"\")))\n",
    "\n",
    "\n",
    "    '''put the path of each of the read me file into first_elts'''\n",
    "    first_elts = [x[0] for x in readmewithname]\n",
    "    #print(first_elts)\n",
    "    '''put the readme file in a list called second_elts'''\n",
    "    #project list\n",
    "    second_elts = [x[1] for x in readmewithname]\n",
    "    \n",
    "    #compute cosine similarity of new and old readme files. this is done by \n",
    "        #comparing each of the old readme with the new readme\n",
    "        \n",
    "    similarityvalues = []\n",
    "    for i in range(len(second_elts)):\n",
    "        sim = cosine_sim(second_elts[i],newProjectReadme[0])\n",
    "        similarityvalues.append(sim)\n",
    "    print(sim)\n",
    "        #print(\"Similarity between old and new Readme %d = %f\"%(i,sim))\n",
    "\n",
    "    #compute the maximum of the similarity value and get the path.\n",
    "    maxi = 0\n",
    "    for i in range(len(similarityvalues)):\n",
    "        if similarityvalues[i] > maxi:\n",
    "            maxi = similarityvalues[i]\n",
    "            maxIndex = i\n",
    "\n",
    "    #ntpath.basename extracts the name of a file from a path\n",
    "    targetp = ntpath.basename(first_elts[maxIndex])\n",
    "\n",
    "    '''Graph of fork activities on github'''\n",
    "    \n",
    "    F= nx.Graph()\n",
    "    fork_headers = ['user_id','projectid']\n",
    "    forks = pd.read_csv('trialdata/fork.csv',header=None,skiprows=1, names=fork_headers)\n",
    "    fusers = forks.user_id.tolist()\n",
    "    fproj = forks.projectid.tolist()\n",
    "    #print(forks.head())\n",
    "    F.add_edges_from(forks.values)\n",
    "\n",
    "    '''Graph of watch or stargazers activities on github'''\n",
    "    W= nx.Graph()\n",
    "    watch_headers = ['user_id','projectid']\n",
    "    watchers = pd.read_csv('trialdata/watchers.csv',header=None,skiprows=1, names=watch_headers)\n",
    "    wusers = watchers.user_id.tolist()\n",
    "    wproj = watchers.projectid.tolist()\n",
    "    #print(watchers.head())\n",
    "    W.add_edges_from(watchers.values)\n",
    "\n",
    "    '''Graph of pullrequests activities on github'''\n",
    "    P= nx.Graph()\n",
    "    pullrequest_headers = ['user_id','projectid']\n",
    "    pullrequest = pd.read_csv('trialdata/pullrequest.csv',header=None,skiprows=1, names=pullrequest_headers)\n",
    "    pusers = pullrequest.user_id.tolist()\n",
    "    pproj = pullrequest.projectid.tolist()\n",
    "    #print(pullrequest.head())\n",
    "    P.add_edges_from(pullrequest.values)\n",
    "\n",
    "    '''Graph of commits activities on github'''\n",
    "    C= nx.Graph()\n",
    "    commit_headers = ['user_id','projectid']\n",
    "    commits = pd.read_csv('trialdata/commits.csv',header=None,skiprows=1, names=commit_headers)\n",
    "    cusers = commits.user_id.tolist()\n",
    "    cproj = commits.projectid.tolist()\n",
    "    #print(commits.head())\n",
    "    C.add_edges_from(commits.values)\n",
    "\n",
    "    '''compute the total users and total projects in the four graphs. using set excludes\n",
    "    repeatition of any user or project\n",
    "    '''\n",
    "    totalusers = set(fusers + wusers + cusers + pusers)\n",
    "    totalprojec = set(fproj + wproj + cproj + pproj)\n",
    "    c = 0\n",
    "    #generate a new directed graph where there is a common edge between the existing four graphs.\n",
    "    '''\n",
    "    This new graph is the graph of all developers that meets up with the four criteria\n",
    "    '''\n",
    "    G = nx.DiGraph()\n",
    "    for i in totalusers:\n",
    "        for j in totalprojec:\n",
    "            if F.has_edge(i,j) and W.has_edge(i,j) and P.has_edge(i,j) and C.has_edge(i,j):\n",
    "                #print('user %s found in (%s, %s)'% (i,i,j))\n",
    "                G.add_edge(i,j)\n",
    "                c = c + 1\n",
    "\n",
    "    occur_users = [u[0] for u in G.edges()]\n",
    "    testusers = occur_users\n",
    "    occur_projects = [u[1] for u in G.edges()]\n",
    "    #count the number project of each user occurence in the trust graph  \n",
    "    usercount = Counter(occur_users)\n",
    "    #print('usercount',\\n')\n",
    "\n",
    "    nx.draw(F,with_labels=True,node_color='g')\n",
    "    plt.savefig('fork graph')\n",
    "    #plt.show()\n",
    "\n",
    "    nx.draw(W,with_labels=True,node_color='b')\n",
    "    plt.savefig('watch graph')\n",
    "    #plt.show()\n",
    "\n",
    "    nx.draw(G,with_labels = True,node_color='y')\n",
    "    plt.savefig('Trust Graph')\n",
    "    plt.show()\n",
    "    #max(occur_users, key=occur_users.count)\n",
    "\n",
    "    #print( 'Users with the four characteristics to a project ',G.edges() )\n",
    "    #Generic Recommendation. without consideration of experience level \n",
    "    Recommended_users = [u[0] for u in G.in_edges(targetp)]\n",
    "\n",
    "    '''\n",
    "    #normalize the experience level by dividing the number of projects a developer has worked \n",
    "    on by the total projects\n",
    "    occur_projects = projects with developers that met up with the four criteria above\n",
    "    '''\n",
    "    j = dict(usercount)\n",
    "\n",
    "    normUsercount = {}\n",
    "    for key,value in j.items():\n",
    "        normUsercount[key] = value/len(occur_projects)\n",
    "    print(normUsercount)\n",
    "\n",
    "    '''Gets only developers who are involved in the most similar project we have identified earlier\n",
    "    u[0] = userid\n",
    "    u[1] = identified project\n",
    "    QUsers = Qualified developers involved with identified similar project\n",
    "    '''\n",
    "    dicOfQualifiedUsers = {}\n",
    "    for u in G.in_edges(targetp):\n",
    "        dicOfQualifiedUsers[u[0]] = u[1]\n",
    "    \n",
    "    QUsers = [] \n",
    "    for key,value in dicOfQualifiedUsers.items():\n",
    "        QUsers.append(key)\n",
    "\n",
    "    '''\n",
    "    # getting the experience level of only qualified users in the normUserCount experience level above.\n",
    "    key = userid in normUsercount(which is experience level of all users in the new trust graph G)\n",
    "    value = experience level value.\n",
    "    '''  \n",
    "\n",
    "    QUserExp = {}\n",
    "    print('user',' value')\n",
    "    for user in QUsers:\n",
    "        for key,value in normUsercount.items():\n",
    "            if user == key:\n",
    "                print(key,' ',value,'\\n')\n",
    "                QUserExp[user] = value\n",
    "\n",
    "    #appending the concepts of the old readme file that selected users have worked with to the user,\n",
    "    #getting project the Qusers were trusted partakers of \n",
    "    #for x in occur_users,occur_projects:\n",
    "\n",
    "    '''\n",
    "    extract all other projects the Quser or Trusted User has participated in\n",
    "    zip(occur_users,occur_projects) brings list together and makes them a tuple of user-project.\n",
    "    the code below simply converts the user project tuple to a user-project dictionary.\n",
    "    then checks to see if a user in the occured users from graph G is a qualified developer then assign the involved\n",
    "    projet as key, value pair repespectively\n",
    "    '''\n",
    "    QUserProj = {}\n",
    "    for user, project in zip(occur_users, occur_projects):\n",
    "        if user in QUserExp:\n",
    "            QUserProj[user] = project\n",
    "    '''\n",
    "    read in the programming languages of projects.\n",
    "    #convert the projects and programming language into 'dicti'\n",
    "    key = projects\n",
    "    value = list of programming languages used in a project\n",
    "    l2[1:]=  takes off all the keys in the programming language (prolang) dictionary which is project names\n",
    "    and returns only the list of programming languages involoved in it.\n",
    "    'p'+l2[0] = appends a p to the id of each project.\n",
    "    '''\n",
    "    prolang = []\n",
    "    with open(r\"paper/githubProjLang/languages.csv\",'r') as Prolang:\n",
    "        reader = csv.reader(Prolang)\n",
    "        for row in reader:\n",
    "            prolang.append(row)\n",
    "    dicti = {}\n",
    "    for l2 in prolang:\n",
    "        dicti['p'+l2[0]] = l2[1:]\n",
    "\n",
    "    '''\n",
    "    generate a programming language profile of each qualified developer in quserproj.\n",
    "    QUsersprofilecount counts the number of programming languages a developer has been involved in.\n",
    "    '''\n",
    "    QUsersProfile = {}\n",
    "    for key,value in QUserProj.items():\n",
    "        for key1,value1 in dicti.items():\n",
    "            if value==key1:\n",
    "                QUsersProfile[key] = value1\n",
    "    QUsersProfileCount = {}\n",
    "    for key,value in QUsersProfile.items():\n",
    "        QUsersProfileCount[key] = len(value)\n",
    "\n",
    "    '''\n",
    "    read in the programming languages used in the new project which we are recommending developers\n",
    "    newdicti is a dictionary containing the new project name as key and the list of programming languages as value\n",
    "    '''\n",
    "    newprolang = []\n",
    "    with open(\"paper/newprojectreadme/pnewlang.csv\",'r') as NewProlang:\n",
    "        reader = csv.reader(NewProlang)\n",
    "        for row in reader:\n",
    "            newprolang.append(row)\n",
    "    newdicti = {}\n",
    "    for l2 in newprolang:\n",
    "        newdicti[l2[0]] = l2[1:]\n",
    "    print(newprolang,newdicti)\n",
    "\n",
    "    '''\n",
    "    evaluate the number of languages each user has that is common to the new project's programming languages.\n",
    "    '''\n",
    "    langsim = {}\n",
    "    for key,value in QUsersProfile.items():\n",
    "        lang_sim = set(value).intersection(set(newdicti['pnew']))\n",
    "        langsim[key] = len(lang_sim)/len(set(newdicti['pnew']))\n",
    "    '''\n",
    "    compute total relevance or trust level a project has for each qualified developer by summing up languages\n",
    "    similarity score and experience level\n",
    "    '''\n",
    "    totalRel = {}\n",
    "    for key,value in QUserExp.items():\n",
    "        for k1,v2 in langsim.items():\n",
    "            if key in langsim and k1 in QUserExp:\n",
    "                totalRel[key] = QUserExp[key] + langsim[key]            \n",
    "\n",
    "    '''\n",
    "    Rank the users based on their total relevance level or projectTrust level.\n",
    "    top n = 2, developers are recommended. by slicing down the ranked recommendation list\n",
    "    '''\n",
    "    #sorted(totalRel,key=totalRel.get, reverse=True)\n",
    "    Recommendation_list = {key: rank for rank, key in enumerate(\\\n",
    "                                sorted(totalRel, key=totalRel.get, reverse=True), 1)}\n",
    "    n_recomm = dicslice(2,Recommendation_list.items())\n",
    "    predicted_users = [x for x in Recommendation_list]\n",
    "\n",
    "\n",
    "    '''\n",
    "    Test for accuracy which is the correctly recommended users from the testusers list in the first notebook\n",
    "    '''\n",
    "    count = 0\n",
    "    real_predicted = []\n",
    "    for user in predicted_users:\n",
    "        if user in testusers:\n",
    "            count = count + 1\n",
    "            real_predicted.append(user)\n",
    "    acurracy = count/len(set(testusers))\n",
    "\n",
    "    print('Maximum similarity value is %f index %d'%(maxi,maxIndex))\n",
    "    print('Similar project is ',ntpath.basename(first_elts[maxIndex]))\n",
    "    print('total relevance level',totalRel)\n",
    "    print('Recommendation List =',Recommendation_list)\n",
    "    print('Top two recommended developers for the new project',n_recomm)\n",
    "    print('Recommended developers = ',predicted_users)\n",
    "    print('testusers',testusers)\n",
    "    print('realpredicted',real_predicted)\n",
    "    print('Recommendation Acurracy = ',acurracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_punctuation_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d011d82112e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mProjectTrust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p52'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-45d1e188d8b1>\u001b[0m in \u001b[0;36mProjectTrust\u001b[0;34m(targetP)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0msimilarityvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_elts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_elts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewProjectReadme\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0msimilarityvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-45d1e188d8b1>\u001b[0m in \u001b[0;36mcosine_sim\u001b[0;34m(text1, text2)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mProjectTrust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nkorojoseph/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nkorojoseph/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nkorojoseph/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nkorojoseph/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-45d1e188d8b1>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m'''remove punctuation, lowercase, stem'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstem_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_punctuation_map' is not defined"
     ]
    }
   ],
   "source": [
    "ProjectTrust('p52')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['u13', 'u3', 'u5', 'u19', 'u9', 'u1o', 'u4', 'u2', 'u6', 'u10', 'u11', 'u1', 'u7']) dict_keys(['u4', 'u2', 'u6', 'u1'])\n",
      "[0, 0.1346153846153846, 0, 0.28846153846153844, 0, 0.07692307692307693, 0, 0, 0, 0.07692307692307693, 0, 0, 0]\n",
      "[2, 7, 1, 15, 1, 4, 6, 2, 2, 4, 6, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nif8H24IAiSMe0u0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febb006f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Trusted Users and Experience level =  0.886326716698\n"
     ]
    }
   ],
   "source": [
    "keys = set(totalRel.keys()) | set(j.keys())\n",
    "keys1 = list(QUserExp.keys() | j.keys())\n",
    "print(j.keys(), QUserExp.keys())\n",
    "\n",
    "print([QUserExp.get(x,0) for x in keys1])\n",
    "print([j.get(x, 0) for x in keys1])\n",
    "x = [QUserExp.get(x, 0) for x in keys1]\n",
    "y = [j.get(x, 0) for x in keys]\n",
    "plt.scatter(x,y)\n",
    "plt.savefig(testfile+'userexp')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Correlation between Trusted Users and Experience level = ',numpy.corrcoef(\n",
    "    [QUserExp.get(x, 0) for x in keys1],\n",
    "    [j.get(x, 0) for x in keys1])[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['u5', 'u3', 'u10', 'u19', 'u11', 'u13', 'u1', 'u7', 'u9', 'u6', 'u2', 'u1o', 'u4']) dict_keys([])\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[6, 2, 1, 2, 6, 1, 1, 4, 15, 2, 1, 7, 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqRAWuiQVwkKXpEJY6JJUiP8DDTNZ4fT5j6oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2823875048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between programming language similarity and Trusted developers =  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkorojoseph/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3003: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/nkorojoseph/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3004: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keys =  set(langsim.keys()) | set(j.keys())\n",
    "\n",
    "print(j.keys(), langsim.keys())\n",
    "\n",
    "print([langsim.get(x, 0) for x in keys])\n",
    "print([j.get(x, 0) for x in keys])\n",
    "x = [langsim.get(x, 0) for x in keys]\n",
    "y = [j.get(x, 0) for x in keys]\n",
    "plt.scatter(x,y)\n",
    "plt.savefig(testfile+'langsim')\n",
    "plt.show()\n",
    "\n",
    "print('Correlation between programming language similarity and Trusted developers = ',numpy.corrcoef(\n",
    "    [langsim.get(x, 0) for x in keys],\n",
    "    [j.get(x, 0) for x in keys])[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mount Gilead hospital"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
